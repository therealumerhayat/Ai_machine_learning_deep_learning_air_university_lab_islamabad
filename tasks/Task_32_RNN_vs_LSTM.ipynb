{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMdWPwkODPk3"
      },
      "source": [
        "**Name:** Muhammad Umer\n",
        "\n",
        "**Email** umerhayat282@gmail.com\n",
        "\n",
        "**Date** November 30, 2025\n",
        "\n",
        "____"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d210024"
      },
      "source": [
        "## Understanding Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTMs)\n",
        "\n",
        "This notebook aims to provide a clear and concise introduction to Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTMs), two fundamental architectures for processing sequential data. We'll explore their concepts with simple explanations and practical code examples using TensorFlow/Keras.\n",
        "\n",
        "### Why do we need RNNs/LSTMs?\n",
        "Traditional neural networks treat inputs as independent entities. However, many real-world problems involve sequential data where the order matters (e.g., text, speech, time series). RNNs and LSTMs are designed to handle this temporal dependency."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e636f434"
      },
      "source": [
        "## 1. Recurrent Neural Networks (RNNs)\n",
        "\n",
        "### Concept\n",
        "Recurrent Neural Networks (RNNs) are a class of neural networks where connections between nodes form a directed graph along a temporal sequence. This allows them to exhibit temporal dynamic behavior. Unlike feedforward neural networks, RNNs can use their internal state (memory) to process sequences of inputs.\n",
        "\n",
        "The core idea is that an RNN has a 'memory' that captures information about what has been calculated so far. The output at time `t` depends on the input at `t` and the hidden state from `t-1`.\n",
        "\n",
        "Here's a simple unrolled representation:\n",
        "\n",
        "![RNN Unrolled Diagram](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b5/Recurrent_neural_network_unfold.svg/600px-Recurrent_neural_network_unfold.svg.png)\n",
        "\n",
        "*   **$X_t$**: Input at time step `t`\n",
        "*   **$h_t$**: Hidden state at time step `t` (memory of the network)\n",
        "*   **$O_t$**: Output at time step `t`\n",
        "\n",
        "### Limitations\n",
        "One of the main challenges with vanilla RNNs is the **vanishing/exploding gradient problem**, which makes it difficult for them to learn long-range dependencies in sequences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "522deffd"
      },
      "source": [
        "### RNN Code Example: Simple Sequence Prediction\n",
        "Let's create a very simple RNN model to predict the next number in a sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecc83603",
        "outputId": "a763a5d1-82bd-4531-ec46-28e9e1053f64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train shape: (1000, 3, 1)\n",
            "y_train shape: (1000,)\n",
            "Example X_train[0]: [83 84 85] -> y_train[0]: 86\n",
            "\n",
            "Training RNN model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RNN model trained.\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step\n",
            "\n",
            "Input sequence: [10, 11, 12]\n",
            "RNN Predicted next number: 11.66\n",
            "Actual next number: 13\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense\n",
        "\n",
        "# 1. Prepare Data\n",
        "# We'll create a simple sequence: [0, 1, 2, 3, 4, 5, ...]\n",
        "# Input: current number, Output: next number\n",
        "\n",
        "def generate_sequence_data(num_samples, seq_length):\n",
        "    X, y = [], []\n",
        "    for _ in range(num_samples):\n",
        "        start = np.random.randint(0, 100)\n",
        "        sequence = [start + i for i in range(seq_length + 1)]\n",
        "        X.append(sequence[:-1]) # Input sequence (e.g., [0, 1, 2])\n",
        "        y.append(sequence[-1])  # Target (e.g., 3)\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "seq_length = 3\n",
        "num_samples = 1000\n",
        "X_train, y_train = generate_sequence_data(num_samples, seq_length)\n",
        "\n",
        "# RNNs expect input in (batch_size, timesteps, features)\n",
        "# Our input is (num_samples, seq_length) -> need to add a feature dimension\n",
        "X_train = X_train.reshape(num_samples, seq_length, 1)\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}\") # (1000, 3, 1)\n",
        "print(f\"y_train shape: {y_train.shape}\") # (1000,)\n",
        "print(\"Example X_train[0]:\", X_train[0].flatten(), \"-> y_train[0]:\", y_train[0])\n",
        "\n",
        "# 2. Build the RNN Model\n",
        "model_rnn = Sequential([\n",
        "    SimpleRNN(units=10, activation='relu', input_shape=(seq_length, 1)),\n",
        "    Dense(units=1)\n",
        "])\n",
        "\n",
        "model_rnn.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# 3. Train the Model\n",
        "print(\"\\nTraining RNN model...\")\n",
        "history_rnn = model_rnn.fit(X_train, y_train, epochs=10, verbose=0)\n",
        "print(\"RNN model trained.\")\n",
        "\n",
        "# 4. Make a Prediction\n",
        "# Let's predict the next number for [10, 11, 12]\n",
        "new_sequence = np.array([10, 11, 12]).reshape(1, seq_length, 1)\n",
        "predicted_rnn = model_rnn.predict(new_sequence)[0][0]\n",
        "\n",
        "print(f\"\\nInput sequence: {[10, 11, 12]}\")\n",
        "print(f\"RNN Predicted next number: {predicted_rnn:.2f}\")\n",
        "print(f\"Actual next number: {13}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd3dd6aa"
      },
      "source": [
        "## 2. Long Short-Term Memory (LSTM)\n",
        "\n",
        "### Concept\n",
        "Long Short-Term Memory (LSTM) networks are a special kind of RNN, capable of learning long-term dependencies. They were introduced to address the vanishing gradient problem that plagues traditional RNNs.\n",
        "\n",
        "Instead of a single neural network layer in the repeating module of an RNN, LSTMs have four interacting layers that communicate in a very special way. The key to LSTMs is the **cell state**, which acts like a conveyor belt carrying information throughout the sequence. Information can be added to or removed from the cell state by structures called **gates**.\n",
        "\n",
        "There are three main types of gates:\n",
        "1.  **Forget Gate**: Decides what information to throw away from the cell state.\n",
        "2.  **Input Gate**: Decides what new information to store in the cell state.\n",
        "3.  **Output Gate**: Decides what to output based on the cell state.\n",
        "\n",
        "![LSTM Diagram](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)\n",
        "\n",
        "LSTMs are very effective for tasks like speech recognition, machine translation, and time series forecasting where long-term memory is crucial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed8d6a4d"
      },
      "source": [
        "### LSTM Code Example: Simple Sequence Prediction\n",
        "Let's use the same sequence prediction task to demonstrate an LSTM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cb81b06",
        "outputId": "134714f5-3107-49fe-ce63-b09a544d872d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training LSTM model...\n",
            "LSTM model trained.\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 269ms/step\n",
            "\n",
            "Input sequence: [10, 11, 12]\n",
            "LSTM Predicted next number: 12.00\n",
            "Actual next number: 13\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "# 1. Prepare Data (same as RNN example)\n",
        "# X_train, y_train are already generated above.\n",
        "# X_train shape: (num_samples, seq_length, 1)\n",
        "\n",
        "# 2. Build the LSTM Model\n",
        "model_lstm = Sequential([\n",
        "    LSTM(units=10, activation='relu', input_shape=(seq_length, 1)),\n",
        "    Dense(units=1)\n",
        "])\n",
        "\n",
        "model_lstm.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# 3. Train the Model\n",
        "print(\"Training LSTM model...\")\n",
        "history_lstm = model_lstm.fit(X_train, y_train, epochs=10, verbose=0)\n",
        "print(\"LSTM model trained.\")\n",
        "\n",
        "# 4. Make a Prediction\n",
        "# Using the same sequence: [10, 11, 12]\n",
        "predicted_lstm = model_lstm.predict(new_sequence)[0][0]\n",
        "\n",
        "print(f\"\\nInput sequence: {[10, 11, 12]}\")\n",
        "print(f\"LSTM Predicted next number: {predicted_lstm:.2f}\")\n",
        "print(f\"Actual next number: {13}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57a4d867"
      },
      "source": [
        "## 3. RNN vs. LSTM: Key Differences\n",
        "\n",
        "Here's a quick comparison of the two architectures:\n",
        "\n",
        "| Feature           | Recurrent Neural Network (RNN)             | Long Short-Term Memory (LSTM)                     |\n",
        "| :---------------- | :----------------------------------------- | :------------------------------------------------ |\n",
        "| **Memory**        | Simple loop/recurrent connection           | Complex 'cell state' with gates                   |\n",
        "| **Long-term dependencies** | Struggles with vanishing/exploding gradients; difficult to capture | Explicitly designed to capture long-term dependencies through gates |\n",
        "| **Complexity**    | Simpler architecture, fewer parameters     | More complex architecture, more parameters        |\n",
        "| **Training Speed**| Generally faster per epoch                 | Generally slower per epoch (due to more computations) |\n",
        "| **Use Cases**     | Short sequences, simpler patterns          | Long sequences, complex patterns (speech, translation) |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34d5804f"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "*   **RNNs** are foundational for sequence processing but suffer from vanishing gradients, limiting their ability to learn long-term dependencies.\n",
        "*   **LSTMs** overcome these limitations with their sophisticated gate mechanisms and cell state, making them highly effective for tasks requiring long-term memory. While more complex, they are often the go-to choice for many real-world sequential data problems.\n",
        "\n",
        "This notebook provided a basic understanding and implementation. For more complex tasks, you would typically use larger datasets, more sophisticated preprocessing, and deeper networks."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
